# ğŸ”¹ AÃ§Ä±klama: Taxi-v3 ortamÄ±nda Q-learning eÄŸitimi. Gym ve NumPy 2.0 sÃ¼rÃ¼mlerine uyumlu hale getirilmiÅŸtir.
# ğŸ”¹ Gerekli pip paketleri: pip install gym numpy tqdm

import gym
import numpy as np
import random
from tqdm import tqdm

if not hasattr(np, "bool8"):
    np.bool8 = bool  # gym bazÄ± yerlerde np.bool8 kullanÄ±yor

def get_state_from_reset(reset_return):
    """gym.reset() bazÄ± sÃ¼rÃ¼mlerde sadece state (obs) dÃ¶ner,
    bazÄ± sÃ¼rÃ¼mlerde (state, info) dÃ¶ner."""
    if isinstance(reset_return, (tuple, list)):
        return reset_return[0]
    return reset_return

def step_env(env, action):
    """env.step() farklÄ± gym sÃ¼rÃ¼mlerinde 4-tuple veya 5-tuple dÃ¶nebilir."""
    ret = env.step(action)
    if len(ret) == 4:
        next_state, reward, done, info = ret
        return next_state, reward, done, info
    elif len(ret) == 5:
        next_state, reward, terminated, truncated, info = ret
        done = terminated or truncated
        return next_state, reward, done, info
    else:
        next_state = ret[0]
        reward = ret[1] if len(ret) > 1 else 0
        done = bool(ret[2]) if len(ret) > 2 else False
        info = ret[-1] if len(ret) > 0 else {}
        return next_state, reward, done, info

# Ortam oluÅŸturma
env = gym.make("Taxi-v3", render_mode="ansi")
state = get_state_from_reset(env.reset())

# Render (bazÄ± sÃ¼rÃ¼mler liste dÃ¶ndÃ¼rebiliyor)
try:
    rendered = env.render()
    if isinstance(rendered, list) and len(rendered) > 0:
        print(rendered[0])
    else:
        print(rendered)
except Exception:
    pass

"""
Hareket kodlarÄ± (Taxi-v3):
0: gÃ¼ney
1: kuzey
2: doÄŸu
3: batÄ±
4: yolcuyu almak
5: yolcuyu bÄ±rak
"""

action_space = env.action_space.n
state_space = env.observation_space.n

q_table = np.zeros((state_space, action_space))

alpha = 0.1  # Ã¶ÄŸrenme oranÄ±
gamma = 0.6  # iskonto oranÄ±
epsilon = 0.1  # keÅŸif oranÄ±

# EÄŸitim dÃ¶ngÃ¼sÃ¼
for i in tqdm(range(1, 100001)):
    state = get_state_from_reset(env.reset())
    done = False
    
    while not done:
        # %10 keÅŸif, %90 sÃ¶mÃ¼rÃ¼
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()
        else:
            action = int(np.argmax(q_table[state]))
    
        next_state, reward, done, info = step_env(env, action)
        
        # Q-table gÃ¼ncelleme
        q_table[state, action] = q_table[state, action] + alpha * (
            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]
        )
        
        state = next_state
        
print("Training finished âœ…")

# Test bÃ¶lÃ¼mÃ¼
total_epoch, total_penalties = 0, 0
episodes = 100

for i in tqdm(range(episodes)):
    state = get_state_from_reset(env.reset())
    epochs, penalties, reward = 0, 0, 0
    done = False
    
    while not done:
        action = int(np.argmax(q_table[state]))
        next_state, reward, done, info = step_env(env, action)
                
        state = next_state
        
        if reward == -10:
            penalties += 1
            
        epochs += 1
    
    total_epoch += epochs
    total_penalties += penalties
    
print(f"Result after {episodes} episodes")
print("Average timesteps per episode:", total_epoch / episodes)
print("Average penalties per episode:", total_penalties / episodes)
